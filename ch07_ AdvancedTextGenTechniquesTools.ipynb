{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashishkhurana01/Hands-On-LLM/blob/main/ch07_%20AdvancedTextGenTechniquesTools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUcXxb07nrYZ"
      },
      "source": [
        "# Advanced Text Gen Tools"
      ],
      "id": "hUcXxb07nrYZ"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FmPTq29UnrYa"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>=0.1.6 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2 langchain_community\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python==0.2.69"
      ],
      "id": "FmPTq29UnrYa"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an LLM\n",
        "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk5C16cXqwC9",
        "outputId": "e0072248-7b96-4dd2-e40a-4fdc2bc26904"
      },
      "id": "kk5C16cXqwC9",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-27 02:51:00--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.55, 18.164.174.118, 18.164.174.17, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1737949860&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzk0OTg2MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=EczKDJXHXKfZAqZYdZTElr52tufcQdDoGwRQON1Xfa7ch3PV1okzredVdoncM1BUm5QtKAj-lJxM3U1yY8jDREfQKJeaM1lntdAsSjnIYlri-A5eyppIuhGpx2mn%7EG-NSRwvl7b2yjFGjZm%7E0I6eyF4a22eLQ3D5XUeSQFGbo4-Eo1hMhtKk2SJZ4Omi9bowKPOdW%7E8dEy4s5Kp8B1G-ecaMUMwMLgMV5LGRJ66V%7EjtXQ2H4jif6qS%7EvTX8F1zyMjDAymeJS0d%7ELnnHLHQFfG5tsDM--u4FIVBVlrV5qpEf3v1DG3CcWof1itQS-w3VgZFx0zUd3MVWsSTM2Eu8wyw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-01-27 02:51:00--  https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1737949860&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzk0OTg2MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=EczKDJXHXKfZAqZYdZTElr52tufcQdDoGwRQON1Xfa7ch3PV1okzredVdoncM1BUm5QtKAj-lJxM3U1yY8jDREfQKJeaM1lntdAsSjnIYlri-A5eyppIuhGpx2mn%7EG-NSRwvl7b2yjFGjZm%7E0I6eyF4a22eLQ3D5XUeSQFGbo4-Eo1hMhtKk2SJZ4Omi9bowKPOdW%7E8dEy4s5Kp8B1G-ecaMUMwMLgMV5LGRJ66V%7EjtXQ2H4jif6qS%7EvTX8F1zyMjDAymeJS0d%7ELnnHLHQFfG5tsDM--u4FIVBVlrV5qpEf3v1DG3CcWof1itQS-w3VgZFx0zUd3MVWsSTM2Eu8wyw__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.164.174.19, 18.164.174.52, 18.164.174.98, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.164.174.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7643295904 (7.1G) [binary/octet-stream]\n",
            "Saving to: ‘Phi-3-mini-4k-instruct-fp16.gguf’\n",
            "\n",
            "Phi-3-mini-4k-instr 100%[===================>]   7.12G  40.0MB/s    in 3m 1s   \n",
            "\n",
            "2025-01-27 02:54:02 (40.2 MB/s) - ‘Phi-3-mini-4k-instruct-fp16.gguf’ saved [7643295904/7643295904]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LlamaCpp\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=2048,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "j7eSx2PprR64"
      },
      "id": "j7eSx2PprR64",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8ZQWC7YgrlaO",
        "outputId": "b4f332c7-074d-4045-e8af-550b624307aa"
      },
      "id": "8ZQWC7YgrlaO",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chains**"
      ],
      "metadata": {
        "id": "u93mb3HTr0H4"
      },
      "id": "u93mb3HTr0H4"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# Create a prompt template with the \"input_prompt\" variable\n",
        "template = \"\"\"<s><|user|>\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\"]\n",
        ")"
      ],
      "metadata": {
        "id": "aOY5IAPQr4LA"
      },
      "id": "aOY5IAPQr4LA",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_chain = prompt | llm"
      ],
      "metadata": {
        "id": "8DgMHUC8ug4C"
      },
      "id": "8DgMHUC8ug4C",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the chain\n",
        "basic_chain.invoke(\n",
        "    {\n",
        "        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8h8x7WE4umwi",
        "outputId": "6ed1260e-2ecc-48e0-9dec-0447a24ea4b0"
      },
      "id": "8h8x7WE4umwi",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hello Maarten, the answer to 1 + 1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple Chains**"
      ],
      "metadata": {
        "id": "H4-Pc0-Xvc1G"
      },
      "id": "H4-Pc0-Xvc1G"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "# Create a chain for the title of our story\n",
        "template = \"\"\"<s><|user|>\n",
        "Create a title for a story about {summary}. Only return the title.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
        "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeUyJaF6vf0A",
        "outputId": "116a9ad5-5b9d-4104-db2e-8ec555009769"
      },
      "id": "oeUyJaF6vf0A",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-61dd782c6da9>:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title.invoke({\"summary\": \"a girl that lost her way\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPLLKfqH2BtJ",
        "outputId": "299dc1ef-cf11-425a-e325-a68c4217637c"
      },
      "id": "uPLLKfqH2BtJ",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a girl that lost her way',\n",
              " 'title': ' \"Lost Paths: A Journey of Self-Discovery\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chain for the character description using the summary and title\n",
        "template = \"\"\"<s><|user|>\n",
        "Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "character_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\"]\n",
        ")\n",
        "character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")"
      ],
      "metadata": {
        "id": "ht-S-NXw2IQt"
      },
      "id": "ht-S-NXw2IQt",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chain for the story using the summary, title, and character description\n",
        "template = \"\"\"<s><|user|>\n",
        "Create a story about {summary} with the title {title}. The main charachter is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "story_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
        ")\n",
        "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")"
      ],
      "metadata": {
        "id": "qKTKjZ582aFI"
      },
      "id": "qKTKjZ582aFI",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all three components to create the full chain\n",
        "llm_chain = title | character | story"
      ],
      "metadata": {
        "id": "6g1Rz2PO2fD-"
      },
      "id": "6g1Rz2PO2fD-",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke(\"a girl that lost her way\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orfSXlsE2iJV",
        "outputId": "e356ac2c-4bff-42ce-85e3-54c7f3e29e95"
      },
      "id": "orfSXlsE2iJV",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a girl that lost her way',\n",
              " 'title': ' \"Trapped in Transition: The Journey of Lily, Lost and Found\"',\n",
              " 'character': \" Lily is an intelligent and resilient young woman who faces adversity head-on as she navigates through life's challenges following a traumatic event that leaves her feeling lost and disconnected from the world around her. With courage in her heart, she embarks on a journey of self-discovery to find her purpose and reconnect with those she loves while learning valuable lessons about love, friendship, and resilience along the way.\",\n",
              " 'story': ' \"Trapped in Transition: The Journey of Lily, Lost and Found\" is a poignant tale that follows Lily, an intelligent and resilient young woman who finds herself navigating life\\'s tumultuous challenges after enduring a traumatic event. Feeling lost and disconnected from the world around her, she embarks on a courageous journey of self-discovery to find purpose and reconnect with loved ones while learning invaluable lessons about love, friendship, and resilience along the way. Through each twist and turn, Lily\\'s tenacity shines through as she overcomes obstacles, confronts her fears, and ultimately emerges stronger than ever before, finding solace and belonging within herself while forging lasting connections with those who stand by her side.'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Memory\n",
        "# Let's give the LLM our name\n",
        "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
      ],
      "metadata": {
        "id": "zFf8yc9k250Q",
        "outputId": "557964eb-730e-42ee-fde4-e6651a36ec92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "zFf8yc9k250Q",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hello Maarten! The answer to 1 + 1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we ask the LLM to reproduce the name\n",
        "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ],
      "metadata": {
        "id": "CqPqswsP3BJl",
        "outputId": "7b832230-4f38-4e92-efa0-b08e7c5a1ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "id": "CqPqswsP3BJl",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm unable to determine your personal information, including your name, as I don't have the capability to access or retrieve personal data. How can I assist you with general queries instead?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conversation Buffer Memory\n",
        "#  Create an updated prompt template to include a chat history\n",
        "template = \"\"\"<s><|user|>Current conversation:{chat_history}\n",
        "\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")"
      ],
      "metadata": {
        "id": "GUAyDn2a3e9L"
      },
      "id": "GUAyDn2a3e9L",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Define the type of Memory we will use\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# Chain the LLM, Prompt, and Memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "3iZka8oa4E-4",
        "outputId": "7f41d3e6-d1d4-4e50-c776-b4f7abfa9765",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3iZka8oa4E-4",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-9823403f20ac>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a conversation and ask a basic question\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
      ],
      "metadata": {
        "id": "EppE9yf74TLG",
        "outputId": "9d6f0e96-4d81-413b-a0cd-7cc6709f0e97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EppE9yf74TLG",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'Hi! My name is Maarten. What is 1 + 1?',\n",
              " 'chat_history': '',\n",
              " 'text': \" The sum of 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another unit, resulting in two units total.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Does the LLM remember the name we gave it?\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ],
      "metadata": {
        "id": "PZfaI_5z4Ydo",
        "outputId": "20e120ee-bb1b-4d9d-f4b3-f090c4364fcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PZfaI_5z4Ydo",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten. What is 1 + 1?\\nAI:  The sum of 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another unit, resulting in two units total.\",\n",
              " 'text': ' Your name is Maarten.\\n\\nAs for the math question, the answer is that 1 + 1 equals 2.'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ConversationBufferMemoryWindow\n",
        "\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# Retain only the last 2 conversations in memory\n",
        "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
        "\n",
        "# Chain the LLM, Prompt, and Memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "_qKS64Kr4vP6",
        "outputId": "eb046638-6c71-4ec5-93b6-1aea16e4f400",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_qKS64Kr4vP6",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-3d1e4337015b>:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask two questions and generate two conversations in its memory\n",
        "llm_chain.invoke({\"input_prompt\":\"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\"})\n",
        "llm_chain.invoke({\"input_prompt\":\"What is 3 + 3?\"})"
      ],
      "metadata": {
        "id": "jU2PR6Qh42Ur",
        "outputId": "dddd8564-b1f5-4fdd-e248-13eb033e4e68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "jU2PR6Qh42Ur",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is 3 + 3?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  Hello Maarten! It's a pleasure to meet you. To answer your question, 1 + 1 equals 2.\\n\\nHowever, if we take this as an opportunity for further interaction:\\n\\nHi Maarten! I'm glad to connect with you too. You asked about basic arithmetic earlier—I hope that was just a friendly icebreaker and not something you needed assistance with urgently. Is there any other topic or question on your mind?\",\n",
              " 'text': ' Hello Maarten! Nice meeting you as well. Regarding the math question, 3 + 3 equals 6. If you have any other topics or questions in mind, feel free to ask!'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether it knows the name we gave it\n",
        "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
      ],
      "metadata": {
        "id": "TBWDob72489Q",
        "outputId": "c95e97fb-2f15-4a6d-d944-db113e415be4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TBWDob72489Q",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  Hello Maarten! It's a pleasure to meet you. To answer your question, 1 + 1 equals 2.\\n\\nHowever, if we take this as an opportunity for further interaction:\\n\\nHi Maarten! I'm glad to connect with you too. You asked about basic arithmetic earlier—I hope that was just a friendly icebreaker and not something you needed assistance with urgently. Is there any other topic or question on your mind?\\nHuman: What is 3 + 3?\\nAI:  Hello Maarten! Nice meeting you as well. Regarding the math question, 3 + 3 equals 6. If you have any other topics or questions in mind, feel free to ask!\",\n",
              " 'text': \" Your name is Maarten. It's a pleasure to interact with you! I don't recall this specific interaction before, but if we were continuing our conversation from earlier, your question about 3 + 3 would yield the answer of 6. If there's anything else you'd like to discuss or any other questions, feel free to ask!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether it knows the age we gave it\n",
        "llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"
      ],
      "metadata": {
        "id": "25-D-ye-5FbP",
        "outputId": "7b7738d6-2429-4a4a-89da-c6fe85ef4ee5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "25-D-ye-5FbP",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my age?',\n",
              " 'chat_history': \"Human: What is 3 + 3?\\nAI:  Hello Maarten! Nice meeting you as well. Regarding the math question, 3 + 3 equals 6. If you have any other topics or questions in mind, feel free to ask!\\nHuman: What is my name?\\nAI:  Your name is Maarten. It's a pleasure to interact with you! I don't recall this specific interaction before, but if we were continuing our conversation from earlier, your question about 3 + 3 would yield the answer of 6. If there's anything else you'd like to discuss or any other questions, feel free to ask!\",\n",
              " 'text': \" I'm sorry, but I don't have access to personal data such as your age. However, if you need assistance with calculating ages in a hypothetical scenario or understanding how age-related concepts work, feel free to ask!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}